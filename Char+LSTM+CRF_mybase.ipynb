{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import (pack_padded_sequence, pad_packed_sequence, pad_sequence)\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数设置\n",
    "class config(object):\n",
    "    ftrain = 'data/ctb5/train.conll'\n",
    "    fdev = 'data/ctb5/dev.conll'\n",
    "    ftest = 'data/ctb5/test.conll'\n",
    "    fembed = 'data/sgns.renmin.char'\n",
    "    n_context = 1\n",
    "    n_embed = 300\n",
    "    n_hidden = 150\n",
    "    drop = 0.5\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    interval = 10\n",
    "    eta = 0.001\n",
    "    file = 'network_char_LSTM_CRF.pt'\n",
    "    use_char = True\n",
    "    n_char_out = 200\n",
    "    n_hidden = 150\n",
    "    n_char_embed = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 正态分布初始化嵌入向量矩阵\n",
    "def init_embedding(tensor):\n",
    "    std = (1. / tensor.size(1)) ** 0.5\n",
    "    nn.init.normal_(tensor, mean=0, std=std)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Corpus(object):\n",
    "    PAD = '<PAD>'\n",
    "    UNK = '<UNK>'\n",
    "    SOS = '<SOS>'\n",
    "    EOS = '<EOS>'\n",
    "\n",
    "    def __init__(self, fdata, fembed=None):\n",
    "        # 获取数据的句子\n",
    "        self.sents = self.preprocess(fdata)\n",
    "        # 获取数据的所有不同的词汇、词性和字符\n",
    "        self.words, self.tags, self.chars = self.parse(self.sents)\n",
    "        # 增加句首词汇、句尾词汇、填充词汇和未知词汇\n",
    "        self.words = [self.PAD, self.UNK, self.SOS, self.EOS] + self.words\n",
    "        # 增加填充字符和未知字符\n",
    "        self.chars = [self.PAD, self.UNK] + self.chars\n",
    "\n",
    "        # 词汇字典\n",
    "        self.wdict = {w: i for i, w in enumerate(self.words)}\n",
    "        # 词性字典\n",
    "        self.tdict = {t: i for i, t in enumerate(self.tags)}\n",
    "        # 字符字典\n",
    "        self.cdict = {c: i for i, c in enumerate(self.chars)}\n",
    "\n",
    "        # 填充词汇索引\n",
    "        self.pad_wi = self.wdict[self.PAD]\n",
    "        # 未知词汇索引\n",
    "        self.unk_wi = self.wdict[self.UNK]\n",
    "        # 句首词汇索引\n",
    "        self.sos_wi = self.wdict[self.SOS]\n",
    "        # 句尾词汇索引\n",
    "        self.sos_wi = self.wdict[self.EOS]\n",
    "        # 填充字符索引\n",
    "        self.pad_ci = self.cdict[self.PAD]\n",
    "        # 未知字符索引\n",
    "        self.unk_ci = self.cdict[self.UNK]\n",
    "\n",
    "        # 句子数量\n",
    "        self.n_sents = len(self.sents)\n",
    "        # 词汇数量\n",
    "        self.n_words = len(self.words)\n",
    "        # 词性数量\n",
    "        self.n_tags = len(self.tags)\n",
    "        # 字符数量\n",
    "        self.n_chars = len(self.chars)\n",
    "\n",
    "        # 预训练词嵌入\n",
    "        self.embed = self.get_embed(fembed) if fembed is not None else None\n",
    "\n",
    "    def extend(self, words):\n",
    "        unk_words = [w for w in words if w not in self.wdict]\n",
    "        unk_chars = [c for c in ''.join(unk_words) if c not in self.cdict]\n",
    "        # 扩展词汇和字符\n",
    "        self.words = sorted(set(self.words + unk_words) - {self.PAD})\n",
    "        self.chars = sorted(set(self.chars + unk_chars) - {self.PAD})\n",
    "        self.words = [self.PAD] + self.words\n",
    "        self.chars = [self.PAD] + self.chars\n",
    "        # 更新字典\n",
    "        self.wdict = {w: i for i, w in enumerate(self.words)}\n",
    "        self.cdict = {c: i for i, c in enumerate(self.chars)}\n",
    "        # 更新索引\n",
    "        self.pad_wi = self.wdict[self.PAD]\n",
    "        self.unk_wi = self.wdict[self.UNK]\n",
    "        self.sos_wi = self.wdict[self.SOS]\n",
    "        self.sos_wi = self.wdict[self.EOS]\n",
    "        self.pad_ci = self.cdict[self.PAD]\n",
    "        self.unk_ci = self.cdict[self.UNK]\n",
    "        # 更新词汇和字符数\n",
    "        self.n_words = len(self.words)\n",
    "        self.n_chars = len(self.chars)\n",
    "\n",
    "    def load(self, fdata, use_char=False, n_context=1, max_len=10):\n",
    "        sentences = self.preprocess(fdata)\n",
    "        x, y, char_x, lens = [], [], [], []\n",
    "\n",
    "        for wordseq, tagseq in sentences:\n",
    "            #找不到就返回未登录词的序号\n",
    "            wiseq = [self.wdict.get(w, self.unk_wi) for w in wordseq]\n",
    "            tiseq = [self.tdict[t] for t in tagseq]\n",
    "            # 获取每个词汇的上下文\n",
    "            if n_context > 1:\n",
    "                x.append(self.get_context(wiseq, n_context))\n",
    "            else:\n",
    "                x.append(torch.tensor(wiseq, dtype=torch.long))\n",
    "            y.append(torch.tensor(tiseq, dtype=torch.long))\n",
    "            # 不足最大长度的部分用0填充\n",
    "            # 估计没有长度大于10个字的词？\n",
    "            # 注意一下的w[:10]不会溢出\n",
    "            char_x.append(torch.tensor([\n",
    "                [self.cdict.get(c, self.unk_ci)\n",
    "                 for c in w[:max_len]] + [0] * (max_len - len(w))\n",
    "                for w in wordseq\n",
    "            ]))\n",
    "            lens.append(len(tiseq))\n",
    "\n",
    "        x = pad_sequence(x, True)\n",
    "        y = pad_sequence(y, True)\n",
    "        char_x = pad_sequence(char_x, True)\n",
    "        lens = torch.tensor(lens)\n",
    "\n",
    "        if use_char:\n",
    "            dataset = TensorDataset(x, y, char_x, lens)\n",
    "        else:\n",
    "            dataset = TensorDataset(x, y, lens)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_context(self, wiseq, n_context):\n",
    "        half = n_context // 2\n",
    "        length = len(wiseq)\n",
    "        wiseq = [self.sos_wi] * half + wiseq + [self.sos_wi] * half\n",
    "        context = [wiseq[i:i + n_context] for i in range(length)]\n",
    "        context = torch.tensor(context, dtype=torch.long)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def get_embed(self, fembed):\n",
    "        with open(fembed, 'r') as f:\n",
    "            lines = [line for line in f]\n",
    "        splits = [line.split() for line in lines]\n",
    "        splits = splits[1:]\n",
    "        # 获取预训练数据中的词汇和嵌入矩阵\n",
    "        words, embed = zip(*[\n",
    "            (split[0], list(map(float, split[1:]))) for split in splits\n",
    "        ])\n",
    "        # 扩充词汇\n",
    "        self.extend(words)\n",
    "        # 初始化词嵌入\n",
    "        embed = torch.tensor(embed, dtype=torch.float)\n",
    "        embed_indices = [self.wdict[w] for w in words]\n",
    "        extended_embed = torch.Tensor(self.n_words, embed.size(1))\n",
    "        init_embedding(extended_embed)\n",
    "        extended_embed[embed_indices] = embed\n",
    "\n",
    "        return extended_embed\n",
    "\n",
    "    def __repr__(self):\n",
    "        info = f\"{self.__class__.__name__}(\\n\"\n",
    "        info += f\"{'':2}num of sentences: {self.n_sents}\\n\"\n",
    "        info += f\"{'':2}num of words: {self.n_words}\\n\"\n",
    "        info += f\"{'':2}num of tags: {self.n_tags}\\n\"\n",
    "        info += f\"{'':2}num of chars: {self.n_chars}\\n\"\n",
    "        info += f\")\\n\"\n",
    "\n",
    "        return info\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(fdata):\n",
    "        start = 0\n",
    "        sentences = []\n",
    "        with open(fdata, 'r') as f:\n",
    "            lines = [line for line in f]\n",
    "        for i, line in enumerate(lines):\n",
    "            if len(lines[i]) <= 1:\n",
    "                splits = [l.split()[1:4:2] for l in lines[start:i]]\n",
    "                wordseq, tagseq = zip(*splits)\n",
    "                start = i + 1\n",
    "                while start < len(lines) and len(lines[start]) <= 1:\n",
    "                    start += 1\n",
    "                sentences.append((wordseq, tagseq))\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(sentences):\n",
    "        wordseqs, tagseqs = zip(*sentences)\n",
    "        words = sorted(set(w for wordseq in wordseqs for w in wordseq))\n",
    "        tags = sorted(set(t for tagseq in tagseqs for t in tagseq))\n",
    "        chars = sorted(set(''.join(words)))\n",
    "\n",
    "        return words, tags, chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理步骤：\n",
    "1.获取词典：（训练集句子中词去重）∪（Embedding预训练词）+ 'PAD' + 'UNK' + 'SOS' + 'EOS'\n",
    "2.获取目标词典：有多少词性就有多少目标\n",
    "2.重构词嵌入矩阵：词嵌入矩阵 = 原词嵌入矩阵 + 正态分布初始化在训练集中但是不在Embedding预训练词集矩阵\n",
    "3.构建训练数据，按词典索引构成训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class CharLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_char, n_embed, n_out):\n",
    "        super(CharLSTM, self).__init__()\n",
    "\n",
    "        # 字嵌入\n",
    "        self.embed = nn.Embedding(num_embeddings=n_char,\n",
    "                                  embedding_dim=n_embed)\n",
    "        # 字嵌入LSTM层\n",
    "        self.lstm = nn.LSTM(input_size=n_embed,\n",
    "                            hidden_size=n_out // 2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x:char_x\n",
    "        B, T = x.shape# [packed_(B*T),10]\n",
    "        # 获取掩码\n",
    "        mask = x.gt(0)# [packed_(B*T),10]\n",
    "        # 获取按长度有序的字序列索引\n",
    "        # 降序排列，字数越多的词越靠前\n",
    "        lens, indices = torch.sort(mask.sum(dim=1), descending=True)#[packed_(B*T),10]->[1,packed_(B*T)]\n",
    "        # 获取逆序索引\n",
    "        #因为lens需要降序，indices需要升序\n",
    "        _, inverse_indices = indices.sort()#[1,packed_(B*T)]\n",
    "        # 获取单词最大长度\n",
    "        max_len = lens[0]\n",
    "        # 序列按长度由大到小排列\n",
    "        x = x[indices, :max_len]#[packed_(B*T),max_len]\n",
    "        # 获取字嵌入向量\n",
    "        x = self.embed(x)#[packed_(B*T),max_len]->[packed_(B*T),max_len,n_embed]\n",
    "        # 打包数据\n",
    "        x = pack_padded_sequence(x, lens, True)#[packed_(B*T),max_len,n_embed]->[packed_(B*T),packed(max_len),n_embed]\n",
    "        #只需要最后一个单元的输出\n",
    "        x, (hidden, _) = self.lstm(x)#[packed_(B*T),packed(max_len),n_embed]->[packed_(B*T),1,n_out]\n",
    "        # 获取词的字符表示\n",
    "        # torch.unbind(hidden)\n",
    "        reprs = torch.cat(torch.unbind(hidden), dim=1)#[packed_(B*T),1,n_out]->[packed_(B*T),n_out]\n",
    "        # 恢复原有的顺序\n",
    "        reprs = reprs[inverse_indices]\n",
    "\n",
    "        return reprs#[B*T,n_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class CHAR_LSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, n_char, n_char_embed, n_char_out,\n",
    "                 n_vocab, n_embed, n_hidden, n_out, drop=0.5):\n",
    "        super(CHAR_LSTM_CRF, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        # 字嵌入LSTM层\n",
    "        self.char_lstm = CharLSTM(n_char=n_char,\n",
    "                                  n_embed=n_char_embed,\n",
    "                                  n_out=n_char_out)\n",
    "\n",
    "        # 词嵌入LSTM层\n",
    "        self.word_lstm = nn.LSTM(input_size=n_embed + n_char_out,\n",
    "                                 hidden_size=n_hidden,\n",
    "                                 batch_first=True,\n",
    "                                 bidirectional=True)\n",
    "\n",
    "        # 输出层\n",
    "        self.out = nn.Linear(n_hidden * 2, n_out)\n",
    "        # CRF层\n",
    "        self.crf = CRF(n_out)\n",
    "\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def load_pretrained(self, embed):\n",
    "        self.embed = nn.Embedding.from_pretrained(embed, False)\n",
    "\n",
    "    def forward(self, x, char_x, lens):\n",
    "        B, T = x.shape\n",
    "        # 获取掩码\n",
    "        mask = x.gt(0)#[B,T]\n",
    "        # 获取词嵌入向量\n",
    "        x = self.embed(x)#[B,T]->[B,T,n_embed]\n",
    "\n",
    "        # 获取字嵌入向量\n",
    "        # char_x -> char_x[mask] [B,T,10]->[packed_(B*T),10]\n",
    "        char_x = self.char_lstm(char_x[mask])#[packed_(B*T),10]->[packed_(B*T),n_out]\n",
    "        # torch.split(char_x, lens.tolist())按照lens.tolist切分  n_out=n_char_out\n",
    "        char_x = pad_sequence(torch.split(char_x, lens.tolist()), True)#[packed_(B*T),n_out]->\n",
    "        #[B,packed_(T),n_out]->[B,T,n_char_out]\n",
    "\n",
    "        # 获取词表示与字表示的拼接\n",
    "        x = torch.cat((x, char_x), dim=-1)#[B,T,n_embed]+[B,T,n_char_out]->[B,T,n_embed+n_char_out]\n",
    "        x = self.drop(x)# \n",
    "\n",
    "        x = pack_padded_sequence(x, lens, True)#[B,T,n_embed+n_char_out]->[B,packed(T),n_embed+n_char_out]\n",
    "        x, _ = self.word_lstm(x)# [B,packed(T),n_embed+n_char_out]->[B,packed(T),n_hidden]\n",
    "        x, _ = pad_packed_sequence(x, True) #[B,packed(T),n_hidden]->[B,T,n_hidden]\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return self.out(x)#[B,T,n_hidden]->[B,T,N]\n",
    "\n",
    "    def fit(self, train_loader, dev_loader, test_loader,\n",
    "            epochs, interval, eta, file):\n",
    "        # 记录迭代时间\n",
    "        total_time = timedelta()\n",
    "        # 记录最大准确率及对应的迭代次数\n",
    "        max_e, max_acc = 0, 0.0\n",
    "        # 设置优化器为Adam\n",
    "        self.optimizer = optim.Adam(params=self.parameters(), lr=eta)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            start = datetime.now()\n",
    "            # 更新参数\n",
    "            self.update(train_loader)\n",
    "\n",
    "            print(f\"Epoch: {epoch} / {epochs}:\")\n",
    "            loss, train_acc = self.evaluate(train_loader)\n",
    "            print(f\"{'train:':<6} Loss: {loss:.4f} Accuracy: {train_acc:.2%}\")\n",
    "            loss, dev_acc = self.evaluate(dev_loader)\n",
    "            print(f\"{'dev:':<6} Loss: {loss:.4f} Accuracy: {dev_acc:.2%}\")\n",
    "            loss, test_acc = self.evaluate(test_loader)\n",
    "            print(f\"{'test:':<6} Loss: {loss:.4f} Accuracy: {test_acc:.2%}\")\n",
    "            t = datetime.now() - start\n",
    "            print(f\"{t}s elapsed\\n\")\n",
    "            total_time += t\n",
    "\n",
    "            # 保存效果最好的模型\n",
    "            if dev_acc > max_acc:\n",
    "                torch.save(self, file)\n",
    "                max_e, max_acc = epoch, dev_acc\n",
    "            elif epoch - max_e >= interval:\n",
    "                break\n",
    "        print(f\"max accuracy of dev is {max_acc:.2%} at epoch {max_e}\")\n",
    "        print(f\"mean time of each epoch is {total_time / epoch}s\\n\")\n",
    "\n",
    "    def update(self, loader):\n",
    "        # 设置为训练模式\n",
    "        self.train()\n",
    "\n",
    "        # 从加载器中加载数据进行训练\n",
    "        for x, y, char_x, lens in loader:\n",
    "            # 清除梯度\n",
    "            self.optimizer.zero_grad()\n",
    "            # 获取掩码\n",
    "            mask = x.gt(0)\n",
    "            target = y[mask]\n",
    "            # B->batch_size\n",
    "            # T->sentences_lens\n",
    "            # N->Tag_lens\n",
    "            out = self(x, char_x, lens) # [B,T,N]\n",
    "            out = out.transpose(0, 1)  # [T, B, N]\n",
    "            y, mask = y.t(), mask.t()  # [T, B]\n",
    "            loss = self.crf(out, y, mask)\n",
    "            # 计算梯度\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            self.optimizer.step()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader):\n",
    "        # 设置为评价模式\n",
    "        self.eval()\n",
    "\n",
    "        loss, tp, total = 0, 0, 0\n",
    "        # 从加载器中加载数据进行评价\n",
    "        for x, y, char_x, lens in loader:\n",
    "            mask = x.gt(0)\n",
    "            target = y[mask]\n",
    "\n",
    "            out = self.forward(x, char_x, lens)\n",
    "            out = out.transpose(0, 1)  # [T, B, N]\n",
    "            y, mask = y.t(), mask.t()  # [T, B]\n",
    "            predict = self.crf.viterbi(out, mask)\n",
    "            loss += self.crf(out, y, mask)\n",
    "            tp += torch.sum(predict == target).item()\n",
    "            total += lens.sum().item()\n",
    "        loss /= len(loader)\n",
    "\n",
    "        return loss, tp / total\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        x, y, char_x, lens = zip(\n",
    "            *sorted(data, key=lambda x: x[-1], reverse=True)\n",
    "        )\n",
    "        max_len = lens[0]\n",
    "        x = torch.stack(x)[:, :max_len]\n",
    "        y = torch.stack(y)[:, :max_len]\n",
    "        char_x = torch.stack(char_x)[:, :max_len]\n",
    "        lens = torch.tensor(lens)\n",
    "\n",
    "        return x, y, char_x, lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, n_tags):\n",
    "        super(CRF, self).__init__()\n",
    "\n",
    "        # 不同的词性个数\n",
    "        self.n_tags = n_tags\n",
    "        # 句间迁移(FROM->TO)\n",
    "        self.trans = nn.Parameter(torch.Tensor(n_tags, n_tags))\n",
    "        # 句首迁移\n",
    "        self.strans = nn.Parameter(torch.Tensor(n_tags))\n",
    "        # 句尾迁移\n",
    "        self.etrans = nn.Parameter(torch.Tensor(n_tags))\n",
    "\n",
    "        # 初始化参数\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = (1 / self.n_tags) ** 0.5\n",
    "        nn.init.normal_(self.trans, mean=0, std=std)\n",
    "        nn.init.normal_(self.strans, mean=0, std=std)\n",
    "        nn.init.normal_(self.etrans, mean=0, std=std)\n",
    "\n",
    "    def forward(self, emit, target, mask):\n",
    "        T, B, N = emit.shape\n",
    "\n",
    "        logZ = self.get_logZ(emit, mask)\n",
    "        score = self.get_score(emit, target, mask)\n",
    "\n",
    "        return (logZ - score) / B\n",
    "\n",
    "    def get_logZ(self, emit, mask):\n",
    "        T, B, N = emit.shape\n",
    "        #emit[0]->batch中每个word_0的所有发射概率\n",
    "        #strans单独定义，start->所有状态的转移概率\n",
    "        #mask [T, B]\n",
    "        alpha = self.strans + emit[0]  # [B, N]\n",
    "\n",
    "        for i in range(1, T):\n",
    "            trans_i = self.trans.unsqueeze(0)  #升维 [1, N, N] unsqueeze(0)->在0位置加一维\n",
    "            emit_i = emit[i].unsqueeze(1)  # [B, 1, N]\n",
    "            #mask[i]是batch的所有word_0的掩码[1,B]\n",
    "            mask_i = mask[i].unsqueeze(1).expand_as(alpha)  # [B, N]\n",
    "            scores = trans_i + emit_i + alpha.unsqueeze(2)  # [B, N, N]\n",
    "            scores = torch.logsumexp(scores, dim=1)  # [B, N]\n",
    "            alpha[mask_i] = scores[mask_i]\n",
    "\n",
    "        return torch.logsumexp(alpha + self.etrans, dim=1).sum()\n",
    "\n",
    "    def get_score(self, emit, target, mask):\n",
    "        T, B, N = emit.shape\n",
    "        scores = torch.zeros(T, B)\n",
    "\n",
    "        # 加上句间迁移分数\n",
    "        scores[1:] += self.trans[target[:-1], target[1:]]\n",
    "        # 加上发射分数\n",
    "        scores += emit.gather(dim=2, index=target.unsqueeze(2)).squeeze(2)\n",
    "        # 通过掩码过滤分数\n",
    "        # 根据掩码取出分数\n",
    "        score = scores.masked_select(mask).sum()\n",
    "\n",
    "        # 获取序列最后的词性的索引\n",
    "        ends = mask.sum(dim=0).view(1, -1) - 1\n",
    "        # 加上句首迁移分数\n",
    "        score += self.strans[target[0]].sum()\n",
    "        # 加上句尾迁移分数\n",
    "        score += self.etrans[target.gather(dim=0, index=ends)].sum()\n",
    "\n",
    "        return score\n",
    "\n",
    "    def viterbi(self, emit, mask):\n",
    "        T, B, N = emit.shape\n",
    "        lens = mask.sum(dim=0)\n",
    "        delta = torch.zeros(T, B, N)\n",
    "        paths = torch.zeros(T, B, N, dtype=torch.long)\n",
    "\n",
    "        delta[0] = self.strans + emit[0]  # [B, N]\n",
    "\n",
    "        for i in range(1, T):\n",
    "            trans_i = self.trans.unsqueeze(0)  # [1, N, N]\n",
    "            emit_i = emit[i].unsqueeze(1)  # [B, 1, N]\n",
    "            scores = trans_i + emit_i + delta[i - 1].unsqueeze(2)  # [B, N, N]\n",
    "            delta[i], paths[i] = torch.max(scores, dim=1)\n",
    "\n",
    "        predicts = []\n",
    "        for i, length in enumerate(lens):\n",
    "            prev = torch.argmax(delta[length - 1, i] + self.etrans)\n",
    "\n",
    "            predict = [prev]\n",
    "            for j in reversed(range(1, length)):\n",
    "                prev = paths[j, i, prev]\n",
    "                predict.append(prev)\n",
    "            # 反转预测序列并保存\n",
    "            predicts.append(torch.tensor(predict).flip(0))\n",
    "\n",
    "        return torch.cat(predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the max num of threads to 4\n",
      "Set the seed for generating random numbers to 1\n",
      "\n",
      "Corpus(\n",
      "  num of sentences: 16091\n",
      "  num of words: 363463\n",
      "  num of tags: 32\n",
      "  num of chars: 7570\n",
      ")\n",
      "\n",
      "Load the dataset\n",
      "  size of trainset: 16091\n",
      "  size of devset: 803\n",
      "  size of testset: 1910\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ac93a5136b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                           collate_fn=network.collate_fn)\n\u001b[0m\u001b[1;32m     26\u001b[0m dev_loader = DataLoader(dataset=devset,\n\u001b[1;32m     27\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "#print(f\"xx{x}x\")\n",
    "#加f,{}中的x会被变量替换\n",
    "print(f\"Set the max num of threads to 4\\n\"\n",
    "          f\"Set the seed for generating random numbers to 1\\n\")\n",
    "torch.set_num_threads(4)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "config = config()\n",
    "corpus = Corpus(config.ftrain, config.fembed)\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "print(\"Load the dataset\")\n",
    "trainset = corpus.load(config.ftrain, config.use_char, config.n_context)\n",
    "devset = corpus.load(config.fdev, config.use_char, config.n_context)\n",
    "testset = corpus.load(config.ftest, config.use_char, config.n_context)\n",
    "print(f\"{'':2}size of trainset: {len(trainset)}\\n\"\n",
    "          f\"{'':2}size of devset: {len(devset)}\\n\"\n",
    "          f\"{'':2}size of testset: {len(testset)}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_char: 7570\n",
      "n_char_embed: 300\n",
      "n_char_out: 200\n",
      "n_vocab: 363463\n",
      "n_embed: 300\n",
      "n_hidden: 150\n",
      "n_out: 32\n",
      "\n",
      "CHAR_LSTM_CRF(\n",
      "  (embed): Embedding(363463, 300)\n",
      "  (char_lstm): CharLSTM(\n",
      "    (embed): Embedding(7570, 300)\n",
      "    (lstm): LSTM(300, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (word_lstm): LSTM(500, 150, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (crf): CRF()\n",
      "  (drop): Dropout(p=0.5)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"n_char: {corpus.n_chars}\\n\"\n",
    "      f\"n_char_embed: {config.n_char_embed}\\n\"\n",
    "      f\"n_char_out: {config.n_char_out}\\n\"\n",
    "      f\"n_vocab: {corpus.n_words}\\n\"\n",
    "      f\"n_embed: {config.n_embed}\\n\"\n",
    "      f\"n_hidden: {config.n_hidden}\\n\"\n",
    "      f\"n_out: {corpus.n_tags}\\n\")\n",
    "      \n",
    "network = CHAR_LSTM_CRF(n_char=corpus.n_chars,\n",
    "                        n_char_embed=config.n_char_embed,\n",
    "                        n_char_out=config.n_char_out,\n",
    "                        n_vocab=corpus.n_words,\n",
    "                        n_embed=config.n_embed,\n",
    "                        n_hidden=config.n_hidden,\n",
    "                        n_out=corpus.n_tags,\n",
    "                        drop=config.drop)\n",
    "      \n",
    "network.load_pretrained(corpus.embed)\n",
    "\n",
    "train_loader = DataLoader(dataset=trainset,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=network.collate_fn)\n",
    "dev_loader = DataLoader(dataset=devset,\n",
    "                        batch_size=config.batch_size,\n",
    "                        collate_fn=network.collate_fn)\n",
    "test_loader = DataLoader(dataset=testset,\n",
    "                         batch_size=config.batch_size,\n",
    "                         collate_fn=network.collate_fn)\n",
    "      \n",
    "print(f\"{network}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Adam optimizer to train the network\n",
      "  epochs: 100\n",
      "  batch_size: 64\n",
      "  interval: 10\n",
      "  eta: 0.001\n",
      "\n",
      "Epoch: 1 / 100:\n",
      "train: Loss: 5.2232 Accuracy: 93.50%\n",
      "dev:   Loss: 5.6749 Accuracy: 92.62%\n",
      "test:  Loss: 5.7739 Accuracy: 92.28%\n",
      "0:07:08.224233s elapsed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CHAR_LSTM_CRF. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CharLSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CRF. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 / 100:\n",
      "train: Loss: 3.4835 Accuracy: 95.61%\n",
      "dev:   Loss: 4.4637 Accuracy: 94.20%\n",
      "test:  Loss: 4.5649 Accuracy: 93.83%\n",
      "0:07:30.987861s elapsed\n",
      "\n",
      "Epoch: 3 / 100:\n",
      "train: Loss: 2.7322 Accuracy: 96.58%\n",
      "dev:   Loss: 4.1193 Accuracy: 94.83%\n",
      "test:  Loss: 4.1604 Accuracy: 94.42%\n",
      "0:07:50.441108s elapsed\n",
      "\n",
      "Epoch: 4 / 100:\n",
      "train: Loss: 2.2918 Accuracy: 97.11%\n",
      "dev:   Loss: 4.0381 Accuracy: 94.78%\n",
      "test:  Loss: 3.9432 Accuracy: 94.61%\n",
      "0:07:31.074870s elapsed\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8da5ae5f1e61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             file=config.file)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-0a5a0566c618>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, dev_loader, test_loader, epochs, interval, eta, file)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# 更新参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch} / {epochs}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0a5a0566c618>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# 计算梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;31m# 更新参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Use Adam optimizer to train the network\")\n",
    "print(f\"{'':2}epochs: {config.epochs}\\n\"\n",
    "      f\"{'':2}batch_size: {config.batch_size}\\n\"\n",
    "      f\"{'':2}interval: {config.interval}\\n\"\n",
    "      f\"{'':2}eta: {config.eta}\\n\")\n",
    "network.fit(train_loader=train_loader,\n",
    "            dev_loader=dev_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=config.epochs,\n",
    "            interval=config.interval,\n",
    "            eta=config.eta,\n",
    "            file=config.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  Loss: 3.6933 Accuracy: 95.39%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-9917aa1ab8d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'test:':<6} Loss: {loss:.4f} Accuracy: {accuracy:.2%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{datetime.now() - start}s elapsed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "network = torch.load(config.file)\n",
    "loss, accuracy = network.evaluate(test_loader)\n",
    "print(f\"{'test:':<6} Loss: {loss:.4f} Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [2,5,67,8]\n",
    "a = a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 67, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
