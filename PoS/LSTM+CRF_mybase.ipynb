{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import (pack_padded_sequence, pad_packed_sequence, pad_sequence)\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    ftrain = 'data/ctb5/train.conll'\n",
    "    fdev = 'data/ctb5/dev.conll'\n",
    "    ftest = 'data/ctb5/test.conll'\n",
    "    fembed = 'data/sgns.renmin.char'\n",
    "    n_context = 1\n",
    "    n_embed = 300\n",
    "    n_hidden = 150\n",
    "    drop = 0.5\n",
    "    batch_size = 50\n",
    "    epochs = 100\n",
    "    interval = 10\n",
    "    eta = 0.001\n",
    "    file = 'network.pt'\n",
    "    use_char = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理步骤：\n",
    "1.获取词典：（训练集句子中词去重）∪（Embedding预训练词）+ 'PAD' + 'UNK' + 'SOS' + 'EOS'\n",
    "2.获取目标词典：有多少词性就有多少目标\n",
    "2.重构词嵌入矩阵：词嵌入矩阵 = 原词嵌入矩阵 + 正态分布初始化在训练集中但是不在Embedding预训练词集矩阵\n",
    "3.构建训练数据，按词典索引构成训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding(tensor):\n",
    "    std = (1. / tensor.size(1)) ** 0.5\n",
    "    nn.init.normal_(tensor, mean=0, std=std)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    PAD = '<PAD>'\n",
    "    UNK = '<UNK>'\n",
    "    SOS = '<SOS>'\n",
    "    EOS = '<EOS>'\n",
    "\n",
    "    def __init__(self, fdata, fembed=None):\n",
    "        # 获取数据的句子\n",
    "        self.sents = self.preprocess(fdata)\n",
    "        # 获取数据的所有不同的词汇、词性和字符\n",
    "        self.words, self.tags, self.chars = self.parse(self.sents)\n",
    "        # 增加句首词汇、句尾词汇、填充词汇和未知词汇\n",
    "        self.words = [self.PAD, self.UNK, self.SOS, self.EOS] + self.words\n",
    "        # 增加填充字符和未知字符\n",
    "        self.chars = [self.PAD, self.UNK] + self.chars\n",
    "\n",
    "        # 词汇字典\n",
    "        self.wdict = {w: i for i, w in enumerate(self.words)}\n",
    "        # 词性字典\n",
    "        self.tdict = {t: i for i, t in enumerate(self.tags)}\n",
    "        # 字符字典\n",
    "        self.cdict = {c: i for i, c in enumerate(self.chars)}\n",
    "\n",
    "        # 填充词汇索引\n",
    "        self.pad_wi = self.wdict[self.PAD]\n",
    "        # 未知词汇索引\n",
    "        self.unk_wi = self.wdict[self.UNK]\n",
    "        # 句首词汇索引\n",
    "        self.sos_wi = self.wdict[self.SOS]\n",
    "        # 句尾词汇索引\n",
    "        self.sos_wi = self.wdict[self.EOS]\n",
    "        # 填充字符索引\n",
    "        self.pad_ci = self.cdict[self.PAD]\n",
    "        # 未知字符索引\n",
    "        self.unk_ci = self.cdict[self.UNK]\n",
    "\n",
    "        # 句子数量\n",
    "        self.n_sents = len(self.sents)\n",
    "        # 词汇数量\n",
    "        self.n_words = len(self.words)\n",
    "        # 词性数量\n",
    "        self.n_tags = len(self.tags)\n",
    "        # 字符数量\n",
    "        self.n_chars = len(self.chars)\n",
    "\n",
    "        # 预训练词嵌入\n",
    "        self.embed = self.get_embed(fembed) if fembed is not None else None\n",
    "\n",
    "    def extend(self, words):\n",
    "        unk_words = [w for w in words if w not in self.wdict]\n",
    "        unk_chars = [c for c in ''.join(unk_words) if c not in self.cdict]\n",
    "        # 扩展词汇和字符\n",
    "        self.words = sorted(set(self.words + unk_words) - {self.PAD})\n",
    "        self.chars = sorted(set(self.chars + unk_chars) - {self.PAD})\n",
    "        self.words = [self.PAD] + self.words\n",
    "        self.chars = [self.PAD] + self.chars\n",
    "        # 更新字典\n",
    "        self.wdict = {w: i for i, w in enumerate(self.words)}\n",
    "        self.cdict = {c: i for i, c in enumerate(self.chars)}\n",
    "        # 更新索引\n",
    "        self.pad_wi = self.wdict[self.PAD]\n",
    "        self.unk_wi = self.wdict[self.UNK]\n",
    "        self.sos_wi = self.wdict[self.SOS]\n",
    "        self.sos_wi = self.wdict[self.EOS]\n",
    "        self.pad_ci = self.cdict[self.PAD]\n",
    "        self.unk_ci = self.cdict[self.UNK]\n",
    "        # 更新词汇和字符数\n",
    "        self.n_words = len(self.words)\n",
    "        self.n_chars = len(self.chars)\n",
    "\n",
    "    def load(self, fdata, use_char=False, n_context=1, max_len=10):\n",
    "        sentences = self.preprocess(fdata)\n",
    "        x, y, char_x, lens = [], [], [], []\n",
    "\n",
    "        for wordseq, tagseq in sentences:\n",
    "            #找不到就返回未登录词的序号\n",
    "            wiseq = [self.wdict.get(w, self.unk_wi) for w in wordseq]\n",
    "            tiseq = [self.tdict[t] for t in tagseq]\n",
    "            # 获取每个词汇的上下文\n",
    "            if n_context > 1:\n",
    "                x.append(self.get_context(wiseq, n_context))\n",
    "            else:\n",
    "                x.append(torch.tensor(wiseq, dtype=torch.long))\n",
    "            y.append(torch.tensor(tiseq, dtype=torch.long))\n",
    "            # 不足最大长度的部分用0填充\n",
    "            char_x.append(torch.tensor([\n",
    "                [self.cdict.get(c, self.unk_ci)\n",
    "                 for c in w[:max_len]] + [0] * (max_len - len(w))\n",
    "                for w in wordseq\n",
    "            ]))\n",
    "            lens.append(len(tiseq))\n",
    "\n",
    "        x = pad_sequence(x, True)\n",
    "        y = pad_sequence(y, True)\n",
    "        char_x = pad_sequence(char_x, True)\n",
    "        lens = torch.tensor(lens)\n",
    "\n",
    "        if use_char:\n",
    "            dataset = TensorDataset(x, y, char_x, lens)\n",
    "        else:\n",
    "            dataset = TensorDataset(x, y, lens)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_context(self, wiseq, n_context):\n",
    "        half = n_context // 2\n",
    "        length = len(wiseq)\n",
    "        wiseq = [self.sos_wi] * half + wiseq + [self.sos_wi] * half\n",
    "        context = [wiseq[i:i + n_context] for i in range(length)]\n",
    "        context = torch.tensor(context, dtype=torch.long)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def get_embed(self, fembed):\n",
    "        with open(fembed, 'r') as f:\n",
    "            lines = [line for line in f]\n",
    "        splits = [line.split() for line in lines]\n",
    "        splits = splits[1:]\n",
    "        # 获取预训练数据中的词汇和嵌入矩阵\n",
    "        words, embed = zip(*[\n",
    "            (split[0], list(map(float, split[1:]))) for split in splits\n",
    "        ])\n",
    "        # 扩充词汇\n",
    "        self.extend(words)\n",
    "        # 初始化词嵌入\n",
    "        embed = torch.tensor(embed, dtype=torch.float)\n",
    "        embed_indices = [self.wdict[w] for w in words]\n",
    "        extended_embed = torch.Tensor(self.n_words, embed.size(1))\n",
    "        init_embedding(extended_embed)\n",
    "        extended_embed[embed_indices] = embed\n",
    "\n",
    "        return extended_embed\n",
    "\n",
    "    def __repr__(self):\n",
    "        info = f\"{self.__class__.__name__}(\\n\"\n",
    "        info += f\"{'':2}num of sentences: {self.n_sents}\\n\"\n",
    "        info += f\"{'':2}num of words: {self.n_words}\\n\"\n",
    "        info += f\"{'':2}num of tags: {self.n_tags}\\n\"\n",
    "        info += f\"{'':2}num of chars: {self.n_chars}\\n\"\n",
    "        info += f\")\\n\"\n",
    "\n",
    "        return info\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(fdata):\n",
    "        start = 0\n",
    "        sentences = []\n",
    "        with open(fdata, 'r') as f:\n",
    "            lines = [line for line in f]\n",
    "        for i, line in enumerate(lines):\n",
    "            if len(lines[i]) <= 1:\n",
    "                splits = [l.split()[1:4:2] for l in lines[start:i]]\n",
    "                wordseq, tagseq = zip(*splits)\n",
    "                start = i + 1\n",
    "                while start < len(lines) and len(lines[start]) <= 1:\n",
    "                    start += 1\n",
    "                sentences.append((wordseq, tagseq))\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(sentences):\n",
    "        wordseqs, tagseqs = zip(*sentences)\n",
    "        words = sorted(set(w for wordseq in wordseqs for w in wordseq))\n",
    "        tags = sorted(set(t for tagseq in tagseqs for t in tagseq))\n",
    "        chars = sorted(set(''.join(words)))\n",
    "\n",
    "        return words, tags, chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class LSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_out, drop=0.5):\n",
    "        super(LSTM_CRF, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        # 词嵌入LSTM层\n",
    "        self.lstm = nn.LSTM(input_size=n_embed,\n",
    "                            hidden_size=n_hidden,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        # 输出层\n",
    "        self.out = nn.Linear(n_hidden * 2, n_out)\n",
    "        # CRF层\n",
    "        self.crf = CRF(n_out)\n",
    "\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def load_pretrained(self, embed):\n",
    "        self.embed = nn.Embedding.from_pretrained(embed, False)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        B, T = x.shape\n",
    "        # 获取词嵌入向量\n",
    "        x = self.embed(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = pack_padded_sequence(x, lens, True)\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = pad_packed_sequence(x, True)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return self.out(x)\n",
    "\n",
    "    def fit(self, train_loader, dev_loader, test_loader,\n",
    "            epochs, interval, eta, file):\n",
    "        # 记录迭代时间\n",
    "        total_time = timedelta()\n",
    "        # 记录最大准确率及对应的迭代次数\n",
    "        max_e, max_acc = 0, 0.0\n",
    "        # 设置优化器为Adam\n",
    "        self.optimizer = optim.Adam(params=self.parameters(), lr=eta)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            start = datetime.now()\n",
    "            # 更新参数\n",
    "            self.update(train_loader)\n",
    "\n",
    "            print(f\"Epoch: {epoch} / {epochs}:\")\n",
    "            loss, train_acc = self.evaluate(train_loader)\n",
    "            print(f\"{'train:':<6} Loss: {loss:.4f} Accuracy: {train_acc:.2%}\")\n",
    "            loss, dev_acc = self.evaluate(dev_loader)\n",
    "            print(f\"{'dev:':<6} Loss: {loss:.4f} Accuracy: {dev_acc:.2%}\")\n",
    "            loss, test_acc = self.evaluate(test_loader)\n",
    "            print(f\"{'test:':<6} Loss: {loss:.4f} Accuracy: {test_acc:.2%}\")\n",
    "            t = datetime.now() - start\n",
    "            print(f\"{t}s elapsed\\n\")\n",
    "            total_time += t\n",
    "\n",
    "            # 保存效果最好的模型\n",
    "            if dev_acc > max_acc:\n",
    "                torch.save(self, file)\n",
    "                max_e, max_acc = epoch, dev_acc\n",
    "            elif epoch - max_e >= interval:\n",
    "                break\n",
    "        print(f\"max accuracy of dev is {max_acc:.2%} at epoch {max_e}\")\n",
    "        print(f\"mean time of each epoch is {total_time / epoch}s\\n\")\n",
    "\n",
    "    def update(self, loader):\n",
    "        # 设置为训练模式\n",
    "        self.train()\n",
    "\n",
    "        # 从加载器中加载数据进行训练\n",
    "        for x, y, lens in loader:\n",
    "            # B(batch_size)\n",
    "            # T(Sentences_size)\n",
    "            # N(Tag_size)\n",
    "            # x [B,T]; y [B,T]\n",
    "            # 清除梯度\n",
    "            self.optimizer.zero_grad()\n",
    "            # 获取掩码\n",
    "            mask = x.gt(0) # [B,T]\n",
    "            target = y[mask]\n",
    "\n",
    "            out = self(x, lens) # [B,T,N]\n",
    "            #转换任意两个维度\n",
    "            out = out.transpose(0, 1)  # [B,T,N]->[T, B, N]\n",
    "            y, mask = y.t(), mask.t()  # [T, B]\n",
    "            # out->发射矩阵\n",
    "            loss = self.crf(out, y, mask)\n",
    "            # 计算梯度\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            self.optimizer.step()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader):\n",
    "        # 设置为评价模式\n",
    "        self.eval()\n",
    "\n",
    "        loss, tp, total = 0, 0, 0\n",
    "        # 从加载器中加载数据进行评价\n",
    "        for x, y, lens in loader:\n",
    "            mask = x.gt(0)\n",
    "            target = y[mask]\n",
    "\n",
    "            out = self.forward(x, lens)\n",
    "            out = out.transpose(0, 1)  # [T, B, N]\n",
    "            y, mask = y.t(), mask.t()  # [T, B]\n",
    "            predict = self.crf.viterbi(out, mask)\n",
    "            loss += self.crf(out, y, mask)\n",
    "            tp += torch.sum(predict == target).item()\n",
    "            total += lens.sum().item()\n",
    "        loss /= len(loader)\n",
    "\n",
    "        return loss, tp / total\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        x, y, lens = zip(\n",
    "            *sorted(data, key=lambda x: x[-1], reverse=True)\n",
    "        )\n",
    "        max_len = lens[0]\n",
    "        x = torch.stack(x)[:, :max_len]\n",
    "        y = torch.stack(y)[:, :max_len]\n",
    "        lens = torch.tensor(lens)\n",
    "\n",
    "        return x, y, lens\n",
    "                \n",
    "\n",
    "class CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, n_tags):\n",
    "        super(CRF, self).__init__()\n",
    "\n",
    "        # 不同的词性个数\n",
    "        self.n_tags = n_tags\n",
    "        # 句间迁移(FROM->TO)\n",
    "        self.trans = nn.Parameter(torch.Tensor(n_tags, n_tags))\n",
    "        # 句首迁移\n",
    "        self.strans = nn.Parameter(torch.Tensor(n_tags))\n",
    "        # 句尾迁移\n",
    "        self.etrans = nn.Parameter(torch.Tensor(n_tags))\n",
    "\n",
    "        # 初始化参数\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = (1 / self.n_tags) ** 0.5\n",
    "        nn.init.normal_(self.trans, mean=0, std=std)\n",
    "        nn.init.normal_(self.strans, mean=0, std=std)\n",
    "        nn.init.normal_(self.etrans, mean=0, std=std)\n",
    "\n",
    "    def forward(self, emit, target, mask):\n",
    "        T, B, N = emit.shape\n",
    "\n",
    "        logZ = self.get_logZ(emit, mask)\n",
    "        score = self.get_score(emit, target, mask)\n",
    "\n",
    "        return (logZ - score) / B\n",
    "\n",
    "    def get_logZ(self, emit, mask):\n",
    "        T, B, N = emit.shape\n",
    "        #emit[0]->batch中每个word_0的所有发射概率\n",
    "        #strans单独定义，start->所有状态的转移概率\n",
    "        #mask [T, B]\n",
    "        alpha = self.strans + emit[0]  # [B, N]\n",
    "\n",
    "        for i in range(1, T):\n",
    "            trans_i = self.trans.unsqueeze(0)  #升维 [1, N, N] unsqueeze(0)->在0位置加一维\n",
    "            emit_i = emit[i].unsqueeze(1)  # [B, 1, N]\n",
    "            #mask[i]是batch的所有word_0的掩码[1,B]\n",
    "            mask_i = mask[i].unsqueeze(1).expand_as(alpha)  # [B, N]\n",
    "            scores = trans_i + emit_i + alpha.unsqueeze(2)  # [B, N, N]\n",
    "            scores = torch.logsumexp(scores, dim=1)  # [B, N]\n",
    "            alpha[mask_i] = scores[mask_i]\n",
    "\n",
    "        return torch.logsumexp(alpha + self.etrans, dim=1).sum()\n",
    "\n",
    "    def get_score(self, emit, target, mask):\n",
    "        T, B, N = emit.shape\n",
    "        scores = torch.zeros(T, B)\n",
    "\n",
    "        # 加上句间迁移分数\n",
    "        scores[1:] += self.trans[target[:-1], target[1:]]\n",
    "        # 加上发射分数\n",
    "        scores += emit.gather(dim=2, index=target.unsqueeze(2)).squeeze(2)\n",
    "        # 通过掩码过滤分数\n",
    "        # 根据掩码取出分数\n",
    "        score = scores.masked_select(mask).sum()\n",
    "\n",
    "        # 获取序列最后的词性的索引\n",
    "        ends = mask.sum(dim=0).view(1, -1) - 1\n",
    "        # 加上句首迁移分数\n",
    "        score += self.strans[target[0]].sum()\n",
    "        # 加上句尾迁移分数\n",
    "        score += self.etrans[target.gather(dim=0, index=ends)].sum()\n",
    "\n",
    "        return score\n",
    "\n",
    "    def viterbi(self, emit, mask):\n",
    "        T, B, N = emit.shape\n",
    "        lens = mask.sum(dim=0)\n",
    "        delta = torch.zeros(T, B, N)\n",
    "        paths = torch.zeros(T, B, N, dtype=torch.long)\n",
    "\n",
    "        delta[0] = self.strans + emit[0]  # [B, N]\n",
    "\n",
    "        for i in range(1, T):\n",
    "            trans_i = self.trans.unsqueeze(0)  # [1, N, N]\n",
    "            emit_i = emit[i].unsqueeze(1)  # [B, 1, N]\n",
    "            scores = trans_i + emit_i + delta[i - 1].unsqueeze(2)  # [B, N, N]\n",
    "            delta[i], paths[i] = torch.max(scores, dim=1)\n",
    "\n",
    "        predicts = []\n",
    "        for i, length in enumerate(lens):\n",
    "            prev = torch.argmax(delta[length - 1, i] + self.etrans)\n",
    "\n",
    "            predict = [prev]\n",
    "            for j in reversed(range(1, length)):\n",
    "                prev = paths[j, i, prev]\n",
    "                predict.append(prev)\n",
    "            # 反转预测序列并保存\n",
    "            predicts.append(torch.tensor(predict).flip(0))\n",
    "\n",
    "        return torch.cat(predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Set the max num of threads to 4\\n\"\n",
    "          f\"Set the seed for generating random numbers to 1\\n\")\n",
    "torch.set_num_threads(4)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "config = config()\n",
    "corpus = Corpus(config.ftrain, config.fembed)\n",
    "print(corpus)\n",
    "\n",
    "print(\"Load the dataset\")\n",
    "trainset = corpus.load(config.ftrain, config.use_char, config.n_context)\n",
    "devset = corpus.load(config.fdev, config.use_char, config.n_context)\n",
    "testset = corpus.load(config.ftest, config.use_char, config.n_context)\n",
    "print(f\"size of trainset: {len(trainset)}\\n\"\n",
    "          f\"size of devset: {len(devset)}\\n\"\n",
    "          f\"size of testset: {len(testset)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Neural Network\n",
      "  n_vocab: 54304\n",
      "  n_embed: 100\n",
      "  n_hidden: 150\n",
      "  n_out: 32\n",
      "\n",
      "LSTM_CRF(\n",
      "  (embed): Embedding(54304, 100)\n",
      "  (lstm): LSTM(100, 150, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=300, out_features=32, bias=True)\n",
      "  (crf): CRF()\n",
      "  (drop): Dropout(p=0.5)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Create Neural Network\")\n",
    "\n",
    "print(f\"n_vocab: {corpus.n_words}\\n\"\n",
    "      f\"n_embed: {config.n_embed}\\n\"\n",
    "      f\"n_hidden: {config.n_hidden}\\n\"\n",
    "      f\"n_out: {corpus.n_tags}\\n\")\n",
    "network = LSTM_CRF(n_vocab=corpus.n_words,\n",
    "                   n_embed=config.n_embed,\n",
    "                   n_hidden=config.n_hidden,\n",
    "                   n_out=corpus.n_tags,\n",
    "                   drop=config.drop)\n",
    "    \n",
    "network.load_pretrained(corpus.embed)\n",
    "\n",
    "train_loader = DataLoader(dataset=trainset,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=network.collate_fn)\n",
    "dev_loader = DataLoader(dataset=devset,\n",
    "                        batch_size=config.batch_size,\n",
    "                        collate_fn=network.collate_fn)\n",
    "test_loader = DataLoader(dataset=testset,\n",
    "                         batch_size=config.batch_size,\n",
    "                         collate_fn=network.collate_fn)\n",
    "      \n",
    "print(f\"{network}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Adam optimizer to train the network\n",
      "  epochs: 100\n",
      "  batch_size: 50\n",
      "  interval: 10\n",
      "  eta: 0.001\n",
      "\n",
      "Epoch: 1 / 100:\n",
      "train: Loss: 7.6402 Accuracy: 90.73%\n",
      "dev:   Loss: 7.4370 Accuracy: 90.19%\n",
      "test:  Loss: 8.2437 Accuracy: 89.46%\n",
      "0:02:38.095340s elapsed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LSTM_CRF. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/anaconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CRF. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 / 100:\n",
      "train: Loss: 5.0652 Accuracy: 93.69%\n",
      "dev:   Loss: 5.4976 Accuracy: 92.46%\n",
      "test:  Loss: 6.1008 Accuracy: 91.82%\n",
      "0:02:33.396411s elapsed\n",
      "\n",
      "Epoch: 3 / 100:\n",
      "train: Loss: 4.0994 Accuracy: 94.81%\n",
      "dev:   Loss: 4.9968 Accuracy: 93.16%\n",
      "test:  Loss: 5.5575 Accuracy: 92.47%\n",
      "0:02:33.330230s elapsed\n",
      "\n",
      "Epoch: 4 / 100:\n",
      "train: Loss: 3.5094 Accuracy: 95.53%\n",
      "dev:   Loss: 4.6367 Accuracy: 93.75%\n",
      "test:  Loss: 5.1702 Accuracy: 92.83%\n",
      "0:02:33.349064s elapsed\n",
      "\n",
      "Epoch: 5 / 100:\n",
      "train: Loss: 3.0532 Accuracy: 96.15%\n",
      "dev:   Loss: 4.4679 Accuracy: 94.06%\n",
      "test:  Loss: 4.7913 Accuracy: 93.31%\n",
      "0:02:34.500253s elapsed\n",
      "\n",
      "Epoch: 6 / 100:\n",
      "train: Loss: 2.7347 Accuracy: 96.51%\n",
      "dev:   Loss: 4.3056 Accuracy: 94.33%\n",
      "test:  Loss: 4.5222 Accuracy: 93.57%\n",
      "0:02:34.068111s elapsed\n",
      "\n",
      "Epoch: 7 / 100:\n",
      "train: Loss: 2.4764 Accuracy: 96.85%\n",
      "dev:   Loss: 4.2183 Accuracy: 94.51%\n",
      "test:  Loss: 4.5454 Accuracy: 93.63%\n",
      "0:02:37.228383s elapsed\n",
      "\n",
      "Epoch: 8 / 100:\n",
      "train: Loss: 2.2742 Accuracy: 97.07%\n",
      "dev:   Loss: 4.1889 Accuracy: 94.53%\n",
      "test:  Loss: 4.4187 Accuracy: 93.79%\n",
      "0:02:35.775963s elapsed\n",
      "\n",
      "Epoch: 9 / 100:\n",
      "train: Loss: 2.0725 Accuracy: 97.34%\n",
      "dev:   Loss: 4.1736 Accuracy: 94.67%\n",
      "test:  Loss: 4.3283 Accuracy: 93.95%\n",
      "0:02:38.576692s elapsed\n",
      "\n",
      "Epoch: 10 / 100:\n",
      "train: Loss: 1.9421 Accuracy: 97.52%\n",
      "dev:   Loss: 4.1403 Accuracy: 94.72%\n",
      "test:  Loss: 4.2935 Accuracy: 93.99%\n",
      "0:02:35.830030s elapsed\n",
      "\n",
      "Epoch: 11 / 100:\n",
      "train: Loss: 1.8066 Accuracy: 97.70%\n",
      "dev:   Loss: 4.1576 Accuracy: 94.72%\n",
      "test:  Loss: 4.2794 Accuracy: 94.07%\n",
      "0:02:38.884697s elapsed\n",
      "\n",
      "Epoch: 12 / 100:\n",
      "train: Loss: 1.6567 Accuracy: 97.87%\n",
      "dev:   Loss: 4.1478 Accuracy: 94.80%\n",
      "test:  Loss: 4.1856 Accuracy: 94.28%\n",
      "0:02:38.685796s elapsed\n",
      "\n",
      "Epoch: 13 / 100:\n",
      "train: Loss: 1.5687 Accuracy: 97.99%\n",
      "dev:   Loss: 4.2139 Accuracy: 94.76%\n",
      "test:  Loss: 4.2569 Accuracy: 94.17%\n",
      "0:02:38.982231s elapsed\n",
      "\n",
      "Epoch: 14 / 100:\n",
      "train: Loss: 1.4811 Accuracy: 98.11%\n",
      "dev:   Loss: 4.1586 Accuracy: 94.84%\n",
      "test:  Loss: 4.1986 Accuracy: 94.29%\n",
      "0:02:38.294310s elapsed\n",
      "\n",
      "Epoch: 15 / 100:\n",
      "train: Loss: 1.3723 Accuracy: 98.25%\n",
      "dev:   Loss: 4.1796 Accuracy: 94.87%\n",
      "test:  Loss: 4.1742 Accuracy: 94.45%\n",
      "0:02:37.351739s elapsed\n",
      "\n",
      "Epoch: 16 / 100:\n",
      "train: Loss: 1.2813 Accuracy: 98.36%\n",
      "dev:   Loss: 4.2671 Accuracy: 94.77%\n",
      "test:  Loss: 4.2366 Accuracy: 94.39%\n",
      "0:02:41.022381s elapsed\n",
      "\n",
      "Epoch: 17 / 100:\n",
      "train: Loss: 1.1991 Accuracy: 98.48%\n",
      "dev:   Loss: 4.2744 Accuracy: 94.74%\n",
      "test:  Loss: 4.2067 Accuracy: 94.48%\n",
      "0:02:43.567508s elapsed\n",
      "\n",
      "Epoch: 18 / 100:\n",
      "train: Loss: 1.1358 Accuracy: 98.56%\n",
      "dev:   Loss: 4.3431 Accuracy: 94.71%\n",
      "test:  Loss: 4.2775 Accuracy: 94.50%\n",
      "0:02:36.049275s elapsed\n",
      "\n",
      "Epoch: 19 / 100:\n",
      "train: Loss: 1.0668 Accuracy: 98.65%\n",
      "dev:   Loss: 4.3519 Accuracy: 94.86%\n",
      "test:  Loss: 4.3391 Accuracy: 94.45%\n",
      "0:02:37.802159s elapsed\n",
      "\n",
      "Epoch: 20 / 100:\n",
      "train: Loss: 0.9958 Accuracy: 98.73%\n",
      "dev:   Loss: 4.4264 Accuracy: 94.83%\n",
      "test:  Loss: 4.4116 Accuracy: 94.43%\n",
      "0:02:38.746881s elapsed\n",
      "\n",
      "Epoch: 21 / 100:\n",
      "train: Loss: 0.9299 Accuracy: 98.83%\n",
      "dev:   Loss: 4.4338 Accuracy: 94.89%\n",
      "test:  Loss: 4.3091 Accuracy: 94.52%\n",
      "0:02:36.881236s elapsed\n",
      "\n",
      "Epoch: 22 / 100:\n",
      "train: Loss: 0.8907 Accuracy: 98.87%\n",
      "dev:   Loss: 4.5404 Accuracy: 94.87%\n",
      "test:  Loss: 4.4706 Accuracy: 94.52%\n",
      "0:02:39.213792s elapsed\n",
      "\n",
      "Epoch: 23 / 100:\n",
      "train: Loss: 0.8410 Accuracy: 98.94%\n",
      "dev:   Loss: 4.6311 Accuracy: 94.81%\n",
      "test:  Loss: 4.5895 Accuracy: 94.43%\n",
      "0:02:37.819302s elapsed\n",
      "\n",
      "Epoch: 24 / 100:\n",
      "train: Loss: 0.7914 Accuracy: 99.01%\n",
      "dev:   Loss: 4.6262 Accuracy: 94.91%\n",
      "test:  Loss: 4.5350 Accuracy: 94.52%\n",
      "0:02:38.607616s elapsed\n",
      "\n",
      "Epoch: 25 / 100:\n",
      "train: Loss: 0.7327 Accuracy: 99.10%\n",
      "dev:   Loss: 4.7024 Accuracy: 94.82%\n",
      "test:  Loss: 4.6169 Accuracy: 94.43%\n",
      "0:02:36.352330s elapsed\n",
      "\n",
      "Epoch: 26 / 100:\n",
      "train: Loss: 0.6837 Accuracy: 99.15%\n",
      "dev:   Loss: 4.7265 Accuracy: 94.76%\n",
      "test:  Loss: 4.5986 Accuracy: 94.56%\n",
      "0:02:43.160758s elapsed\n",
      "\n",
      "Epoch: 27 / 100:\n",
      "train: Loss: 0.6528 Accuracy: 99.20%\n",
      "dev:   Loss: 4.7479 Accuracy: 94.83%\n",
      "test:  Loss: 4.6266 Accuracy: 94.57%\n",
      "0:02:41.328854s elapsed\n",
      "\n",
      "Epoch: 28 / 100:\n",
      "train: Loss: 0.6062 Accuracy: 99.26%\n",
      "dev:   Loss: 4.8429 Accuracy: 94.72%\n",
      "test:  Loss: 4.7559 Accuracy: 94.52%\n",
      "0:02:36.110219s elapsed\n",
      "\n",
      "Epoch: 29 / 100:\n",
      "train: Loss: 0.5805 Accuracy: 99.28%\n",
      "dev:   Loss: 4.8083 Accuracy: 94.88%\n",
      "test:  Loss: 4.7428 Accuracy: 94.47%\n",
      "0:02:40.183866s elapsed\n",
      "\n",
      "Epoch: 30 / 100:\n",
      "train: Loss: 0.5525 Accuracy: 99.32%\n",
      "dev:   Loss: 4.9693 Accuracy: 94.82%\n",
      "test:  Loss: 4.7585 Accuracy: 94.53%\n",
      "0:02:35.935171s elapsed\n",
      "\n",
      "Epoch: 31 / 100:\n",
      "train: Loss: 0.5220 Accuracy: 99.36%\n",
      "dev:   Loss: 4.9641 Accuracy: 94.83%\n",
      "test:  Loss: 4.9643 Accuracy: 94.44%\n",
      "0:02:36.579486s elapsed\n",
      "\n",
      "Epoch: 32 / 100:\n",
      "train: Loss: 0.4707 Accuracy: 99.44%\n",
      "dev:   Loss: 5.0876 Accuracy: 94.79%\n",
      "test:  Loss: 4.8772 Accuracy: 94.48%\n",
      "0:02:36.790803s elapsed\n",
      "\n",
      "Epoch: 33 / 100:\n",
      "train: Loss: 0.4596 Accuracy: 99.45%\n",
      "dev:   Loss: 5.0655 Accuracy: 94.78%\n",
      "test:  Loss: 4.9840 Accuracy: 94.40%\n",
      "0:02:38.509060s elapsed\n",
      "\n",
      "Epoch: 34 / 100:\n",
      "train: Loss: 0.4395 Accuracy: 99.47%\n",
      "dev:   Loss: 5.1968 Accuracy: 94.80%\n",
      "test:  Loss: 5.0168 Accuracy: 94.53%\n",
      "0:02:40.314634s elapsed\n",
      "\n",
      "max accuracy of dev is 94.91% at epoch 24\n",
      "mean time of each epoch is 0:02:37.686017s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Use Adam optimizer to train the network\")\n",
    "print(f\"{'':2}epochs: {config.epochs}\\n\"\n",
    "      f\"{'':2}batch_size: {config.batch_size}\\n\"\n",
    "      f\"{'':2}interval: {config.interval}\\n\"\n",
    "      f\"{'':2}eta: {config.eta}\\n\")\n",
    "network.fit(train_loader=train_loader,\n",
    "            dev_loader=dev_loader,\n",
    "            test_loader=test_loader,\n",
    "            epochs=config.epochs,\n",
    "            interval=config.interval,\n",
    "            eta=config.eta,\n",
    "            file=config.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:  Loss: 5.2197 Accuracy: 94.40%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9917aa1ab8d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'test:':<6} Loss: {loss:.4f} Accuracy: {accuracy:.2%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{datetime.now() - start}s elapsed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "network = torch.load(config.file)\n",
    "loss, accuracy = network.evaluate(test_loader)\n",
    "print(f\"{'test:':<6} Loss: {loss:.4f} Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
