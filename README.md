# zxwNLP
项目对比了LSTM+各类attention机制在IMDB数据集下的文本分类的效果

Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classiﬁcation  
Feed Forward Networks with Attention Can Solve Some Long-Term Memory Problems  
Hierarchical Attention Networks for Document Classiﬁcation  

| Model Name | Accuracy | F1 |
| ------ | ------ | ------ |
| BiLSTM + Attention | 0.9075 | 0.9099 |
| BiLSTM + Feed Forward Networks with Attention | 0.9075 | 0.9094 |
| BiLSTM + Hierarchical Attention | 0.9065 | 0.9085 |
| BiLSTM | 0.8930 | 0.8940 |
